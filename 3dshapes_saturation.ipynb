{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python modules\n",
    "import numpy as np, cv2, sys\n",
    "from matplotlib import pyplot as plt\n",
    "np.set_printoptions(threshold=sys.maxsize, suppress=True)\n",
    "# Load filtered image and label data sampled from 3dshapes_filtered.ipynb\n",
    "images = np.load(\"3dshapesfiltered.npy\")\n",
    "labels = np.load(\"3dshapeslabel.npy\")\n",
    "# Define visualisation function from https://github.com/google-deepmind/3d-shapes\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "    ncols = int(np.ceil(num_images**0.5))\n",
    "    nrows = int(np.ceil(num_images / ncols))\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            ax.imshow(imgs_[ax_i]/255.0, cmap='Greys_r', interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Process training data such that floor/wall/object saturations are equal and drawn from $\\mathrm{Sat}\\sim \\left[\\epsilon, 0.5\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.0 GiB for an array with shape (480000, 64, 64, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_shift \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Training image array\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m images_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m labels_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;241m*\u001b[39mnum_shift, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(images_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 22.0 GiB for an array with shape (480000, 64, 64, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# Initialise fragmentation of saturation shift\n",
    "num_shift = 1000\n",
    "# Training image array\n",
    "images_train = np.zeros(shape=(len(images)*num_shift, 64, 64, 3), dtype=np.float32)\n",
    "labels_train = np.zeros(shape=len(images)*num_shift, dtype=np.float32)\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "for i in range(25):\n",
    "    target_image = images[450, :, :, :]*255.0\n",
    "    target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2HSV)\n",
    "    hue_lower = np.array([210, 0, 0])\n",
    "    hue_upper = np.array([280, 255, 255])\n",
    "    mask = cv2.inRange(target_image, hue_lower, hue_upper)\n",
    "    h, s, v = cv2.split(target_image)\n",
    "    s[mask==255] = 0.5 + 0.8/(2*24.0)*i\n",
    "    target_image = cv2.merge([h, s, v])\n",
    "    target_image = cv2.cvtColor(target_image, cv2.COLOR_HSV2BGR)\n",
    "    image_processed[i, :, :, :] = target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(image_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
